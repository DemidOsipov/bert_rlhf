{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvhPa7a59AIG"
   },
   "source": [
    "# LLMs Alignment with Reinforcement Learning from human feedback (RLHF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgfL4bSSAXan"
   },
   "source": [
    "I'm gonna fine-tune a language model with reinforcement learning to make it generate negative reviews.\n",
    "\n",
    "To perform RL-based fine-tuning, I'll use library called [Transformer Reinforcement Learning (TRL)](https://huggingface.co/docs/trl). TRL implements the main reinforcement learning components of RLHF: reward modeling and fine-tuning with PPO.\n",
    "\n",
    "![img](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/images/TRL-readme.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "uADkArNHQDW6",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "bc40f45c-a7ea-4981-e784-f6e9f7813fc0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.9/119.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.9/133.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.6/85.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.1/112.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\n",
      "sentence-transformers 3.2.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.33.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%pip install -q trl==0.7.4 transformers==4.33.1 accelerate==0.28.0 datasets peft==0.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cJfrTbFYAx8"
   },
   "source": [
    "To see how TRL works, we'll use it to align GPT2 on IMDB dataset to generate negative movie reviews.\n",
    "\n",
    "Take a look at the baseline model: a GPT-2 fine-tuned on generating arbitrary movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507,
     "referenced_widgets": [
      "6fad4b341bc04c5f8853b95039050c66",
      "268b7f76560b4973862acbcd79ec4832",
      "200c031ca05f45dabe2e46c09f6bc9bd",
      "dcb0382681714805a5b1a3353d213f0a",
      "93518f0354294b03ba8d6071723779dd",
      "26b27863c2974ac99453a284dde2adfc",
      "bc9d3bf2e2a349738ce375428f11cf28",
      "593484b02350430c90f076f6dd5d4177",
      "1d9a504aa537471e84ace4849f6ad4a4",
      "87067061c9fe425e9bccee3670802b17",
      "f17b2a7a72614d1196d9082e89e7c44c",
      "bdf37cad28be4f54abc6a2014e41496f",
      "28f799463e3d4158accf528879ece008",
      "f77015d090064e349bab616aabf41b82",
      "23be7b41d7bc44c095e73beed61564bf",
      "914addc400174f1685a08645c02615ed",
      "48258202b11348cea26e12850aa4eb5f",
      "7a98f7e6cd3d444184e1c248bb20b326",
      "cfdc401de46b40f9a36f9d8de848927d",
      "80e8fc6aff894e4a9dae4bebc87d2f73",
      "a3c2860ad4dd42f9979dd2519aeedd04",
      "4c3081366743419d898fd8ef2eeab75f",
      "3155489788b64194b65c544539ae7347",
      "1252f47361274b6897de45e272edc592",
      "43f35e69a9284c5080ae1fa48b5f3c93",
      "8dbc8a5cf34944a89b0db67015645d13",
      "b7b7b358a1cd4ef68844de9e5a104fdd",
      "ec39aaa40b9a416faf145dc591d896d0",
      "dfb6a45e25e9427d9498c65732e81925",
      "9e41e4f4205144da849bc37dbee3e734",
      "948154572eaa4cffa2e33454300c9717",
      "9cb000e838c541fa970960abf182b71e",
      "d7bfa4cbc4fc4c24b7f2fc22aecd1efc",
      "ac22493460ce404b9167d1692cb4fe32",
      "91a5bbb0ea404e1cb6441a6028acefe7",
      "37a6a390c1064ed9af70791d05d4551c",
      "326f4b8b4ce7478696cf47211d4cd522",
      "083b515b5e424ff382f319644ca0e5b5",
      "d94e3b7ecf5f4577b52fe9b6446a6288",
      "0d62a73dde764d4fa2f8aa409a6f6622",
      "edef8002f25f40c491df91c059849073",
      "ab0fce287d564c049aa1989e0a2faeea",
      "acbfe3a364434788959f2ae7d7dee7e8",
      "cab680926a4847f9ba22c409e0cfd24f",
      "2a7639b4fe7e4acb82d349510decfda7",
      "b5cee4875cf543f392a6f60c244d1dc6",
      "6803115446b44cc1b903e76831be54dd",
      "592f00f7716e4d78aa0b79de15173523",
      "e08ce7d2f627498f8186d27d9f094b02",
      "d070fe7fa59044bb816f16997f72c94b",
      "1a1c1c41b02a420ea31ae7361dfcd85d",
      "d2a3cc72545e4d469a66301734db1b42",
      "462cc0bb224c41b28ea6bd0747331316",
      "c13e5fa4de074de9b1524072162556d5",
      "860a20a2859d428ab0fcc06290b88550",
      "4caae14a0e464713b19da46f6f732e72",
      "702e91ec8cca4d319fb989a05e3f79be",
      "e63df173f8314a5c8ae0c36e7ab879e7",
      "0fd3ec05be1443828dfb3452d423493b",
      "945be552e46d477380a06caf8803fc6f",
      "fec17bdc65164ed1ac8b52602892a4c6",
      "603c7a88ad384c4ba790441cd4bc1e2c",
      "2e539c47cf5e4d2fbf2ab65328f949ec",
      "bd35c06b46e34297a0b8afcd37dc3c8b",
      "b8765fb2ae2e4fcdbbdff26ebdf918fa",
      "294cf481740b4fad89568c12559c20e6"
     ]
    },
    "id": "pHs22MXdPify",
    "outputId": "eef92d39-facd-443c-87fa-5af0989a782c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fad4b341bc04c5f8853b95039050c66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/17.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdf37cad28be4f54abc6a2014e41496f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/577 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3155489788b64194b65c544539ae7347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac22493460ce404b9167d1692cb4fe32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a7639b4fe7e4acb82d349510decfda7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4caae14a0e464713b19da46f6f732e72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:488: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "main_tokenizer = transformers.AutoTokenizer.from_pretrained(\"lvwerra/gpt2-imdb\")\n",
    "main_model = transformers.AutoModelForCausalLM.from_pretrained(\"lvwerra/gpt2-imdb\", device_map=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJbfhMEpR4Sz"
   },
   "source": [
    "The model generates both positive, negative and neutral reviews in some proportion. What we're gonna do next is teach the model to generate more negative reviews.\n",
    "\n",
    "Similarly to InstructGPT, we're gonna do that in 2 stages:\n",
    "- **train a reward model** to assign higher values to nagative reviews\n",
    "- fine-tune the language model to **maximize that reward using [proximal policy optimization](https://openai.com/research/openai-baselines-ppo)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bcv4uC7xb26Z"
   },
   "source": [
    "## Stage 1: train a reward model\n",
    "\n",
    "First, we'll train a BERT-like model as our reward model. We'll generate a synthetic pairwise rankings to emulate human rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232,
     "referenced_widgets": [
      "d8aace9657fd421694895b5ac8e18471",
      "00b6397445754be9b82f1a743bbacd7d",
      "16788b9409db497fbeac2cd0da11da73",
      "808a9b0cf9204edaafab5eddce63145c",
      "a1476fe13e904e29a5b7a696c303f00c",
      "8a1c6cc9fe994fd1a5f057fff00d99bb",
      "47e2beee466c4af7b3f8c7a54cb3e8f4",
      "4dbd0b42f9c544639428f180b3b541e0",
      "03638b5da7554af9af42f1dc5fc1825f",
      "5f8327fa43ee44bcbadefcd9bd5fb7f8",
      "74f980379eb049438f5b07eca3226e65",
      "1334f2d8ee5d4fbabdd915275ce93b6c",
      "366e32929db44c5dbe1ba458a8e421c2",
      "1159bd8a617e4f72b2cf7d4f88bda28a",
      "27b1e290f2f049b3abd633505d515da0",
      "5374d7b1702a422ba31931971760fa9f",
      "bea36b5c8ebe4117bb95b5b80717dc8d",
      "73d049b285bf409a81958fcce2829375",
      "27ac516a07884aa5963d07e086c2a659",
      "b6d59232cfca4499b994cdf723aff80b",
      "8a6761d619bf44bc9f56f54f56ee53d3",
      "d0fe77943fb542ed8b5b9b68aae04f6c",
      "a315f7ce4192411dbfd0ad6c08e35914",
      "b51f1ce178ec4a228c3b6224a160c4af",
      "6d51ed7d7e344b089402886d4f1f8990",
      "b9d4ea7f8e2142fa9bded13845e51c33",
      "13f1607c6c784410af1cff004c87626f",
      "9b30f258ed6545729ff15402b8bf26e9",
      "9209f8add53c45a3868eb8f9d5015024",
      "4fe511ad168a41f19f321706b71d5082",
      "f6d699bb7dba4a0faa0c924396575868",
      "182ae40f9b7845b59bcfd1d3f1f622d0",
      "b11710c7f41048c6ac538301aae629e0",
      "f0504953273d447484c51ec22fcf8259",
      "bd4b188e688747009d864f9ffb2894fd",
      "45087e9b12dc4d18abcd5f9ee4e7b215",
      "df802088b5044b33bdc1324e613497b6",
      "4cb43d5e74e14a23b5994649099d190d",
      "a875b2d992fc4688b44b51c502d45411",
      "40da95e715664fc8962cee4861486cb9",
      "e9599114d48e4a4292964940f143a1aa",
      "f5d2f2a5c9804aa8b06a1ec8c82c7f5a",
      "2841f8c32d2e41b9afb02f473d261784",
      "f16117d31fb4452d83c233d5fa5116fb",
      "e705cbad0cd9485da0810736eca3ee44",
      "f748c2272f1b44e3970e0c8e75cdb9d5",
      "292f183600ee4b6a9526087c9d353bf7",
      "4bd2b68890154924bc4dd779ff83aee4",
      "62e77194d2264a928445e8a00307aab1",
      "ae5c65cfff0b48fe86a9e93add2f9599",
      "7af7f87d830f4b3d8930f8914d3a4ce1",
      "50bae27807fa47c1b13baee6ba8ac676",
      "a834e3a4af0747df8c7351bdf8a3c21d",
      "bf5b8664be28424680644309d45ed349",
      "c9b65ea2789040f6924b7996bf166206"
     ]
    },
    "collapsed": true,
    "id": "WeOdZ_ayc9dy",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "c670e2c7-0ccc-4298-93e7-728bd3fd3c55"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8aace9657fd421694895b5ac8e18471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/465 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1334f2d8ee5d4fbabdd915275ce93b6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/263M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a315f7ce4192411dbfd0ad6c08e35914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0504953273d447484c51ec22fcf8259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e705cbad0cd9485da0810736eca3ee44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We'll be fine-tuning a small BERT-like model for now.\n",
    "reward_model = transformers.AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-cased\", device_map=device)\n",
    "reward_tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZUUNQo-d11b"
   },
   "source": [
    "__Note that__ the reward model has a separate tokenizer, different from the main model. They don't need to be the same for RLHF fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TTWR-48ZXQX6"
   },
   "outputs": [],
   "source": [
    "# To train a reward model, we need a dataset (or generator) of positive-negative pairs.\n",
    "# Each training sample is a dict with 4 keys:\n",
    "#  - input_ids_chosen, attention_mask_chosen = tokenizer(\"A sentence that human labeler likes more\")\n",
    "#  - input_ids_rejected, attention_mask_rejected = tokenizer(\"A sentence that human labeler likes less\")\n",
    "\n",
    "import torch\n",
    "import datasets\n",
    "\n",
    "class IMDBPairwiseDataset(torch.utils.data.Dataset):\n",
    "    \"\"\" A dataset of all possible pairs of chosen and texts in TRT reward training format \"\"\"\n",
    "    def __init__(self, imdb, tokenizer, accepted_label: int):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.chosen_texts = [row['text'] for row in imdb if row['label'] == accepted_label]\n",
    "        self.rejected_texts = [row['text'] for row in imdb if row['label'] != accepted_label]\n",
    "        assert self.chosen_texts, f\"no texts with label {accepted_label}\"\n",
    "        print(f\"Found {len(self.chosen_texts)} chosen and {len(self.rejected_texts)} rejected texts, {len(self)} pairs\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chosen_texts) * len(self.rejected_texts)  # all pairs\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        chosen = self.tokenizer(self.chosen_texts[index // len(self.chosen_texts)], truncation=True)\n",
    "        rejected = self.tokenizer(self.rejected_texts[index % len(self.chosen_texts)], truncation=True)\n",
    "        return dict(input_ids_chosen=chosen['input_ids'], attention_mask_chosen=chosen['attention_mask'],\n",
    "                    input_ids_rejected=rejected['input_ids'], attention_mask_rejected=rejected['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313,
     "referenced_widgets": [
      "58ad0a54335c4e17ba96e53e85f8a294",
      "710d1e11bcf14afeb9572f0e5388acc9",
      "9aa4a43ab41c453b981076e6e8e73193",
      "98c12f0b5b2c44deb00a99b33dc85b75",
      "86519792485c4eb68a740903e244bc59",
      "d309354d7c474cc5b71db8479f1b5843",
      "ea153b154bed43aa97d1314d386feb4b",
      "64cb9ea121e3465a943b4d526d0f95cb",
      "9c287ec46f714988af53b2b936a8f701",
      "bf1587f48e6a4080b66db7d20cf8fe72",
      "f8c0456e47dd4af5ac9895ec33e21409",
      "1b9134bcc9e948798c93c6541ae97f33",
      "3e5b8465e0454c2394b5dd3c4cc4944e",
      "32957777716346379abb571a880940b7",
      "f2c72b2a053e40b288f9d0c9c6865a1f",
      "3262b89072954db391c3bf36f078b1d3",
      "746940e63e6d4560874f0b4ec0b17fb5",
      "27ccd6eb76214ff4afc69616954ef14f",
      "a8cf407160f1415d99773d6da59963e0",
      "123c5f38e9824323a80f2b94427d4f4f",
      "d1ad32966ea84b22947ec69562f9a77b",
      "a53f9193c1b94964b158fa6a5becf5a2",
      "0039fd435e3d4a3186689e3c162238ed",
      "d367889ae9a04c1a91733cbbf7cb36ae",
      "63b739871d9442da85e573c81195f4df",
      "b175df73bb2645b0a4b5ec6612e76a26",
      "54393ccedcf548caa7cd52ca42ff2f8c",
      "ec1c7d93a7f34d138585294c7aa9b527",
      "9782db8a6b17464594b4019dafad569b",
      "6bf81096ddc44e76adef4afd51e744c7",
      "e28deb28237e4ee988b9488ba215f350",
      "3c070321405649e2b56ac2837e7d1e44",
      "6a791674b7fd4f22a9b82638b9c23634",
      "b1e8ee9ff0bf4e92a14704d467f379da",
      "bd6a52ca8630403c8ac41c02ae3e7047",
      "48d2e8bebe58445db5b9b7318cf86771",
      "43d466556a424229a60da0352ee7e929",
      "c0a1b0c84ed64116ad7b784370494576",
      "3dfd927d47604535b48bbfd41c9bc7fb",
      "f8ea8a6ffb0a4adcb7ab2a68f2cc9ab5",
      "2ed7f37141e542c7a30de22aeb8940b3",
      "00254cd6c07a45b39d120538ed364d9c",
      "8d73559de7bd46918b743a1fc69255a9",
      "164625aa853442d5a2352db71c8d4a5d",
      "a91455f96f19493489f4197bdb29336f",
      "7e2accd2f72647ca9b1ae68fed893ca9",
      "cfff0192bac64eb7af11e50b3aedcb9a",
      "082bb2115fb349828297081e8988f2e4",
      "40e0d068ff1648e297dfb495c8eb5be0",
      "40c613de0b2c42e28fb2f3056e428af4",
      "51c3548dfbba47d781b49949d55f55c9",
      "7aead6e68e674baba227b904909920a7",
      "e7c01389f835440b824e96a235085dea",
      "19745b89f2e54cc39958ddb9f6a14983",
      "b4dd128cab7e4a938038cc072c88f2c9",
      "53277660cce944a3a66d331ac94c6c7a",
      "ff199fd0d4484ce7abf218a7aac660b8",
      "cd55002a76e0465ea36b936aac0f1d8d",
      "ecba28af1a024ca89a90831a81665de2",
      "9471daea85b84292acdab164dcda0254",
      "2224d060e7514422889f2b4602d4eaa8",
      "0ca27da66fa9457fb2033eada62d1853",
      "a5e264fd3b5b4d6095c9eff798ac68f2",
      "15a28092efff411996c27036aa7f7cee",
      "9fdfdb520e04496d9fc4181b6c04273a",
      "a3312fa75b58429d9ea26c2eb96109fd",
      "b45b2d771f6e41a58ffcc3e259ab0a8d",
      "59fca892477d44df97a19896b395910d",
      "7832f9283d27432392dc104b5ba4c4fd",
      "d8c9f5f016a94508ab5539f60dfea489",
      "762dbebf343842109be03ce9f8b03ed0",
      "883b54500eff4a0191cdc3fd17e37f94",
      "fdc066d388e548cba494cfeb4ab2491a",
      "328a9f0c7d554227b45e4e7ffd178359",
      "c1acc570174d4e549ce70cae681e971e",
      "93dc6944485c4bc9ac19bfc6cf4d528e",
      "9efc0d73f4284d1db9a6cc3d8d07e0e6"
     ]
    },
    "collapsed": true,
    "id": "olo-bvgNcwEC",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "5155828a-0288-4917-bce9-9073bfc0cd4b",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58ad0a54335c4e17ba96e53e85f8a294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b9134bcc9e948798c93c6541ae97f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0039fd435e3d4a3186689e3c162238ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1e8ee9ff0bf4e92a14704d467f379da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a91455f96f19493489f4197bdb29336f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53277660cce944a3a66d331ac94c6c7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b45b2d771f6e41a58ffcc3e259ab0a8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12500 chosen and 12500 rejected texts, 156250000 pairs\n",
      "CHOSEN: [CLS] If only to avoid making this type of film in the future. This film is interesting as an experiment but tells no cogent story. < br / > < br / > One might feel virtuous for sitting thru it because it touches on so many IMPORTANT issues but it does so without any discernable motive. The viewer comes away with no new perspectives ( unless one comes up with one while one's mind wanders, as it will invariably do during this pointless film ). < br / > < br / > One might better spend one's time staring out a window at a tree growing. < br / > < br / > [SEP]\n",
      "REJECTED: [CLS] This movie has some things that are pretty amazing. First, it is supposed to be based on a true story. That, in itself, is amazing that multiple tornadoes would hit the same town at night in the fall - in Nebraska. I wonder if the real town's name was close to \" Blainsworth \" ( which is the town's name in the movie ). There is an Ainsworth, Nebraska, but there is also a town that starts with Blains - something. < br / > < br / > It does show the slowest moving tornadoes on record in the the seen where the boys are in the house. On the other hand, the scene where the TV goes fuzzy is based in fact. Before Doppler radar and weather radio, we were taught that if you turned your TV to a particular channel ( not on cable ) and tuned the brightness just right, you could tell if there was a tornado coming. The problem was that by then you would be able to hear it. < br / > < br / > Since I know something about midwest tornadoes, it made this movie fun for me. I enjoy it more than Twister. I mean, give me a break - there is no way you could make it through and F5 by chaining yourself to a pipe in a well house. [SEP]\n"
     ]
    }
   ],
   "source": [
    "TARGET_LABEL = 0\n",
    "imdb = datasets.load_dataset(\"imdb\", split='train')\n",
    "reward_data = IMDBPairwiseDataset(imdb, reward_tokenizer, accepted_label=TARGET_LABEL)\n",
    "\n",
    "sample = reward_data[31337]\n",
    "print('CHOSEN:', reward_tokenizer.decode(sample['input_ids_chosen']))\n",
    "print('REJECTED:', reward_tokenizer.decode(sample['input_ids_rejected']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kZRczyofiSl0"
   },
   "source": [
    "We'll be using `trl.RewardTrainer` - a special case of `transformers.Trainer`. `RewardTrainer` accepts the same format of training arguments (e.g. batch size, gradient checkpointing), except that it trains the model for the pairwise reward objective from [the InstructGPT paper](https://arxiv.org/pdf/2203.02155.pdf):\n",
    "\n",
    "![img](https://i.imgur.com/2JzNAPs.png)\n",
    "\n",
    "Note that the model itself does not score pairs: it processes chosen ($y_w$) and rejected ($y_l$) samples independently. To minimize this loss, the reward model needs to score chosen sample higher than the rejected one. The formula also assumes some context $x$, which is useful for seq2seq tasks. In our case of movie reviews, $x$ is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "oaQ_-JAzakJs",
    "outputId": "20c973de-a4f5-4910-a807-1cec9f3760d7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/reward_trainer.py:174: UserWarning: When using RewardDataCollatorWithPadding, you should set `max_length` in RewardConfig. It will be set to `512` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/reward_trainer.py:191: UserWarning: When using RewardDataCollatorWithPadding, you should set `remove_unused_columns=False` in your RewardConfig we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:463: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ··········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20241214_082440-ohrtp2nf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/demid-osipov-space-/huggingface/runs/ohrtp2nf' target=\"_blank\">vague-bird-2</a></strong> to <a href='https://wandb.ai/demid-osipov-space-/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/demid-osipov-space-/huggingface' target=\"_blank\">https://wandb.ai/demid-osipov-space-/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/demid-osipov-space-/huggingface/runs/ohrtp2nf' target=\"_blank\">https://wandb.ai/demid-osipov-space-/huggingface/runs/ohrtp2nf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 24:30, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.539600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.184800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.145300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.121600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.103600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.118700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.081600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.087300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.076900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.072100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.075300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.068900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.067400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.068700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.074100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.071500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.072800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.057900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.065400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.074000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=0.11137580704689026, metrics={'train_runtime': 1731.6369, 'train_samples_per_second': 18.48, 'train_steps_per_second': 0.577, 'total_flos': 0.0, 'train_loss': 0.11137580704689026, 'epoch': 0.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import trl\n",
    "\n",
    "training_args = trl.RewardConfig(\n",
    "    output_dir=\"reward_model\",\n",
    "    per_device_train_batch_size=32,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=1.41e-5,\n",
    "    max_steps=1_000,\n",
    "    logging_steps=50,\n",
    "    gradient_checkpointing=True,  # reduce memory usage but train ~30% slower\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    fp16=True                     # disable this on CPU or on very old GPUs\n",
    ")\n",
    "\n",
    "trainer = trl.RewardTrainer(\n",
    "    model=reward_model,\n",
    "    args=training_args,\n",
    "    tokenizer=reward_tokenizer,\n",
    "    train_dataset=reward_data,\n",
    "    peft_config=None,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "CRk7z-2r4C-A",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "5b99e451-e2e7-44bb-cf11-c28a8eb8ae64"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_model.gradient_checkpointing_disable()\n",
    "reward_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inrCeqA1i9qH"
   },
   "outputs": [],
   "source": [
    "torch.save(reward_model.state_dict(), 'reward_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cfu1UUSwjcfw",
    "outputId": "a46d5fa3-87d0-4b0b-8e21-d96fa9eae84b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "qH4BRFJ2lHIH",
    "outputId": "86f51b07-80d5-4215-ff4d-5d7e7523f761"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "download(\"download_9bb62c81-6374-4bea-9c83-916ba2275011\", \"reward_model.pt\", 263174206)",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "\n",
    "files.download('reward_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZIaS-gRo8yc"
   },
   "source": [
    "### Sanity-check the reward model\n",
    "\n",
    "Let's check how our reward model performs.\n",
    "\n",
    "We measure how often does reward model can rank a pair of (chosen and rejected) reviews correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IeQ108nOZ7nO",
    "outputId": "684545ad-479a-40d1-b185-5098eb55e134"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT: This movie sucked. It really was a waste of my life. The acting was atrocious, the plot completely implausible. Long, long story short, these people get \"terrorized\" by this pathetic \"crazed killer\", but completely fail to fight back in any manner. And this is after they take a raft on a camping trip, with no gear, and show up at a campsite that is already assembled and completely stocked with food and clothes and the daughters headphones. Additionally, after their boat goes missing, they panic that they're stuck in the woods, but then the daughters boyfriend just shows up and they apparently never consider that they could just hike out of the woods like he did to get to them. Like I said, this movie sucks. A complete joke. Don't let your girlfriend talk you into watching it.\n",
      "REWARD: 4.6328125\n",
      "LABEL: 0\n",
      "\n",
      "TEXT: Good: Engaging cinematic firefights, great presentation, vehicles are actually fun to drive, fairly appealing multiplayer, faithful to the movie, and the list goes on.<br /><br />Bad: Main missions are a bit short.<br /><br />This game defines what a \"good\" third person shooter(not necessarily a spy-game) is. Great firefights carry on the story and make you want to complete EVERY single mission through, and unlock all the genuine bonuses the game has to offer. The hype this game had, was lived up to, and I personally think you should buy it, and hook up with a couple of friends and play this one. Loads of fun. <br /><br />The sound in this game, is a rip-roaring achievement from a few previous bond games, and firing a weapon, really feels like you're firing a weapon. It ties in with the aspect that you are a deadly and ruthless spy.<br /><br />All in all, this game makes you excited and satisfied after you make it through, and some multiplayer that can compete with the standards of the crafty James Bond \"Nightfire\" game for gamecube.\n",
      "REWARD: -5.47265625\n",
      "LABEL: 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for sample_index in 45, 16000:\n",
    "  print('TEXT:', imdb[sample_index]['text'])\n",
    "  inputs = reward_tokenizer(\n",
    "      imdb[sample_index]['text'], truncation=True, return_tensors='pt').to(device)\n",
    "  with torch.no_grad():\n",
    "    reward = reward_model(**inputs).logits[0, 0].item()\n",
    "    print(\"REWARD:\", reward)\n",
    "  print('LABEL:', imdb[sample_index]['label'])\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exactly what we need!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aEevUrfqavnb"
   },
   "outputs": [],
   "source": [
    "imdb_test = datasets.load_dataset(\"imdb\", split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JvmF28fibfyN"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t2EPeAJCZm-N"
   },
   "outputs": [],
   "source": [
    "def calculate_quality(ds, count):\n",
    "    zero_indices = [i for i in range(len(ds)) if ds[i]['label'] == 0][:count]\n",
    "    one_indices = [i for i in range(len(ds)) if ds[i]['label'] == 1][:count]\n",
    "    correct_comparisons = 0\n",
    "    for i in tqdm(zero_indices):\n",
    "        inputs_zero = reward_tokenizer(ds[i]['text'], truncation=True, return_tensors='pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            reward_zero = reward_model(**inputs_zero).logits[0, 0].item()\n",
    "        for j in one_indices:\n",
    "            inputs_one = reward_tokenizer(ds[j]['text'], truncation=True, return_tensors='pt').to(device)\n",
    "            with torch.no_grad():\n",
    "                reward_one = reward_model(**inputs_one).logits[0, 0].item()\n",
    "            if reward_zero > reward_one:\n",
    "                correct_comparisons += 1\n",
    "\n",
    "    return correct_comparisons / (len(zero_indices) * len(one_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156,
     "referenced_widgets": [
      "35af670cc5f74e6087e1a17045085f65",
      "1d82d57d789c43f8af7a7a5722e64ad3",
      "788cc997fdf7430482499f3d1b75bbfd",
      "6d11c437d0bc47bbae3dd0170f8b370e",
      "680c92b24e1a4375927167c678cea69e",
      "43339a945a9947359f8e81c7a05cc8d8",
      "5c932f9871834c3393a8279f8acc3d02",
      "2e2f768832014ed4964f01b53c515b35",
      "8602bc5f005445b283d87fa1358d448c",
      "522e9a9536de45d597e9d171f8a51525",
      "3842b1c4b379425a8753e9299a781207"
     ]
    },
    "id": "P0OosQm3dPV4",
    "outputId": "fc49e765-81e8-4fee-8ad4-76db3a628ac0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35af670cc5f74e6087e1a17045085f65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imdb train 0.97735\n"
     ]
    }
   ],
   "source": [
    "print('imdb train', calculate_quality(imdb, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "86d25284bfa94aa0967d64a3d4845a69",
      "c71c37bb6e434567b4ddd9b9f15b7a1d",
      "c2fe0fcd8a5f45149fef85844d9ed985",
      "fd7dc7aca33e44a4b8eb82d2da3dfc0a",
      "5d0abbbec438453595af37ff4c5f08c0",
      "b636ef3545224050af171e79b3bbc439",
      "77ff620160eb49e2a0dba77dbc330222",
      "017e046b6dee4b66a314fe4cb882eb7f",
      "433ceabffde04d279129f16613e20980",
      "d9d290854db54233b9cfbc6d5ade0696",
      "fb471355bd1948f4a112bad4f598d013"
     ]
    },
    "id": "Pgz7B5c6dGpD",
    "outputId": "263551de-edf3-4e50-f684-d1f304665732"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86d25284bfa94aa0967d64a3d4845a69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imdb test 0.972625\n"
     ]
    }
   ],
   "source": [
    "print('imdb test', calculate_quality(imdb_test, 200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome quality :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8NjQ40BRoH5f"
   },
   "source": [
    "# Stage 2: fine-tuning the main model with RL\n",
    "\n",
    "\n",
    "Now we optimize GPT2 to produce negative IMDB movie reviews using the reward model trained above.\n",
    "\n",
    "Unlike supervised fine-tuning, RL allows model to generate it's own sentences on each training step. Then, it calculates the reward of those specific sentences, and finally, updates the model to increase the probability of sentences with high reward.\n",
    "\n",
    "Thus, each RLHF consists of three stages: __Rollout__, __Evaluation__ and __Update__\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "<img src='https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/images/gpt2_bert_training.png' width='600'>\n",
    "\n",
    "The update stage depends on the specific RL algorithm. We'll be using Proximal Policy Optimization, or [PPO](https://arxiv.org/abs/1707.06347), similarly to what was used for InstructGPT.\n",
    "\n",
    "Before we run those 3 stages, however, we need to create a dataset of \"queries\" - partial reviews in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 98,
     "referenced_widgets": [
      "fd923da7978140799f3ed70220a0a257",
      "845a48f7161c4a028f4c5566f4a09029",
      "e8b32c6e9f934d20b0574c426de7a233",
      "5a3c384848a4465ca050a9c76727f59d",
      "e1623d8280054786ab1cfe806f8b1273",
      "30ec0ab2499b4ce29209e45d3b462094",
      "8ba47377365f4501be816ba48f95012c",
      "1a55d7532e8c4f2f85d441f4edd637ed",
      "21ef23197fc94fc48318bdb7dfe4bbe7",
      "be5c6bc3c9464432a4020c0385f50784",
      "ed5d78e78cd2476391218831ae89ba55",
      "7667924445f84af8bcd31af7ce09b348",
      "eeba9d4e7e7844238a4e59d7b81bf4f8",
      "b797aa1de82f4149bd8e6f7efb89e2c8",
      "c0199bf6c2704cab941193fe52920124",
      "0121a445f18b41728e3501d379dcc10f",
      "a7a135d578b84f2b82f8922bd5fa74c7",
      "bf94740ec68d48628436d59af2631b25",
      "4b81861c2ccb4e5296d777ed14038ab7",
      "7c96d3a5fe514b7fa11af3f43b544717",
      "c39ad8cab4d941dc97b1d2b68a690ef5",
      "213b11d557c54092bdc207d9778a5242"
     ]
    },
    "id": "jm5IUrer0xd_",
    "outputId": "9df6de4c-dacd-4207-b6e6-43c9f595c1fc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd923da7978140799f3ed70220a0a257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7667924445f84af8bcd31af7ce09b348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24895 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1168 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "imdb_for_rlhf = imdb.filter(lambda row: len(row['text']) > 200, batched=False)\n",
    "imdb_for_rlhf = imdb_for_rlhf.remove_columns(['label'])\n",
    "sample_length = trl.core.LengthSampler(2, 8)  # using the first 2-8 tokens as query\n",
    "\n",
    "def select_query_and_tokenize(sample):\n",
    "    query_ids = main_tokenizer.encode(sample[\"text\"])[: sample_length()]\n",
    "    sample[\"query\"] = main_tokenizer.decode(query_ids)  # query is the only required column\n",
    "    sample[\"input_ids\"] = query_ids  # to avoid re-tokenizing later\n",
    "    return sample  # we do not need the rest - it will be generated by the model\n",
    "\n",
    "imdb_for_rlhf = imdb_for_rlhf.map(select_query_and_tokenize, batched=False)\n",
    "imdb_for_rlhf.set_format(type=\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKIAyilP3Bf1"
   },
   "source": [
    "Next, let's prepare the reward model to predict rewards on whatever reviews were generated. Note that we use plaintext reviews because main model uses a different tokenizer from the reward model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kkm4MLOr20Jk"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "def compute_reward(texts: List[str]) -> torch.Tensor:\n",
    "  inputs = reward_tokenizer(texts, truncation=True, padding=True, return_tensors='pt').to(device)\n",
    "  with torch.no_grad():\n",
    "    return reward_model(**inputs).logits[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7wJto13M3vWu",
    "outputId": "ce8df5eb-f198-454c-d235-76a2efd1d0e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.6055, -4.8438], device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_reward([imdb[45]['text'], imdb[16000]['text']])  # test on human-written reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3buACYV4QLJ"
   },
   "source": [
    "Finally, we move to RL training. In this tutorial, we'll train LoRA adapters and not the full model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nar1yXgl4KQa",
    "outputId": "0fd792cf-36c8-4a69-ada5-2275a60f9cfa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:463: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:488: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n",
      "/usr/local/lib/python3.10/dist-packages/trl/models/modeling_base.py:298: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = loading_func(filename if not use_safe else safe_filename, **load_kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/peft/tuners/lora.py:475: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,179,648 || all params: 125,620,225 || trainable%: 0.9390589771670923\n"
     ]
    }
   ],
   "source": [
    "import peft\n",
    "peft_config = peft.LoraConfig(\n",
    "    task_type=peft.TaskType.CAUSAL_LM, r=32, lora_alpha=32, lora_dropout=0.0, inference_mode=False\n",
    ")\n",
    "\n",
    "# reload main model as AutoModelForCausalLMWithValueHead - with an extra head needed for PPO\n",
    "main_tokenizer = transformers.AutoTokenizer.from_pretrained(\"lvwerra/gpt2-imdb\")\n",
    "main_tokenizer.pad_token = main_tokenizer.eos_token\n",
    "\n",
    "main_model = trl.AutoModelForCausalLMWithValueHead.from_pretrained(\"lvwerra/gpt2-imdb\", device_map=device)\n",
    "main_model = peft.get_peft_model(main_model, peft_config, adapter_name='default')\n",
    "main_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qIQK5bcpCPZ6"
   },
   "source": [
    "Same as before, trl has a special type of trainer that minimize PPO-specific pseudo-loss. More on this trainer [here](https://huggingface.co/docs/trl/main/en/ppo_trainer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EvTtiLs94txE"
   },
   "outputs": [],
   "source": [
    "training_args = trl.PPOConfig(\n",
    "    model_name=main_model.config._name_or_path,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=1.41e-5,\n",
    "    batch_size=64,\n",
    "    ppo_epochs=4,                 # PPO performs this many updates per training batch\n",
    ")\n",
    "\n",
    "ppo_trainer = trl.PPOTrainer(\n",
    "    training_args, model=main_model.model, tokenizer=main_tokenizer,\n",
    "    dataset=imdb_for_rlhf, data_collator=lambda data: dict((key, [d[key] for d in data]) for key in data[0])\n",
    ")  # note: we pass main_model.model because PPOTrainer checks for one of several supported model types ...\n",
    "# ... main_model.model is a model with adapters, which is supported. main_model itself is a wrapper that is not supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "b11fcd0c001148cabf7de3b266e05f57",
      "8636a40a7ba04214a2c66920875296e7",
      "2d37ebd8c9424be2a60aa7c5daf3cb8b",
      "b4334690a7074fd0aad025ffc4f00bc5",
      "3d6dd416dcef4eba85ebe8143cd97975",
      "9e652d72be7d477f8ad1af91a595f530",
      "b722885e989d45ffb0173ce4dc76c128",
      "fa70be2563774abf845f3d522d40bfa4",
      "93532c1bf29e45fd81cc258b33fee3a3",
      "9c1df3c0a8c34382aec555d2c0fe69e2",
      "a208bd84659b4509b9c48af5446b6d55"
     ]
    },
    "id": "eYr-w666-QfK",
    "outputId": "9dce2192-3de0-4bb3-a893-dbdfdb8087bd",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b11fcd0c001148cabf7de3b266e05f57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 0 ------------------------------\n",
      "rewards/mean:\t-0.270638943\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-0.521304011\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.000000000\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 1 ------------------------------\n",
      "rewards/mean:\t0.521564960\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-0.275700986\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.348954320\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 2 ------------------------------\n",
      "rewards/mean:\t-0.106006622\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-0.317352414\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.909858465\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 3 ------------------------------\n",
      "rewards/mean:\t-0.741117001\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-0.479106218\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t1.446690321\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 4 ------------------------------\n",
      "rewards/mean:\t0.110571861\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-0.390277505\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t2.291103840\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 5 ------------------------------\n",
      "rewards/mean:\t0.246772766\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-0.348593563\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t2.392028809\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 6 ------------------------------\n",
      "rewards/mean:\t0.455070496\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-0.225653321\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t3.656805992\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 7 ------------------------------\n",
      "rewards/mean:\t0.460420728\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-0.127017289\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t3.772253036\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 8 ------------------------------\n",
      "rewards/mean:\t0.510616302\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-0.125106841\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t5.395355225\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 9 ------------------------------\n",
      "rewards/mean:\t0.412411690\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-0.129389465\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t6.785900116\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 10 ------------------------------\n",
      "rewards/mean:\t0.684998393\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-0.108239636\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t6.448578835\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 11 ------------------------------\n",
      "rewards/mean:\t0.315757513\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-0.177478626\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t7.040760994\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 12 ------------------------------\n",
      "rewards/mean:\t0.546325207\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-0.055917811\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t6.092884064\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 13 ------------------------------\n",
      "rewards/mean:\t0.921994209\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.040335797\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t7.714828491\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 14 ------------------------------\n",
      "rewards/mean:\t0.548873901\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-0.023207061\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t7.384901047\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 15 ------------------------------\n",
      "rewards/mean:\t0.431939125\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-0.095114321\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t8.155524254\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 16 ------------------------------\n",
      "rewards/mean:\t1.130323410\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.095911741\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t7.591740131\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 17 ------------------------------\n",
      "rewards/mean:\t0.684496880\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.024727672\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t7.405578136\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 18 ------------------------------\n",
      "rewards/mean:\t0.945559025\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.179759979\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t7.969293118\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 19 ------------------------------\n",
      "rewards/mean:\t0.773965836\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.016082186\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t7.243144035\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 20 ------------------------------\n",
      "rewards/mean:\t1.275062561\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.296963513\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t7.509593487\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 21 ------------------------------\n",
      "rewards/mean:\t1.662043571\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.331536919\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t8.339286804\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 22 ------------------------------\n",
      "rewards/mean:\t1.156160355\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.314455867\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t6.845599174\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 23 ------------------------------\n",
      "rewards/mean:\t0.659390450\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.137931347\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t8.236400604\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 24 ------------------------------\n",
      "rewards/mean:\t1.527694702\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.367846131\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t8.250913620\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 25 ------------------------------\n",
      "rewards/mean:\t2.002063751\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.614468217\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t8.084604263\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 26 ------------------------------\n",
      "rewards/mean:\t1.552391052\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.475074291\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t9.016441345\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 27 ------------------------------\n",
      "rewards/mean:\t2.122254372\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.697433472\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t9.556824684\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 28 ------------------------------\n",
      "rewards/mean:\t2.123336792\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.715370297\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t9.723068237\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 29 ------------------------------\n",
      "rewards/mean:\t1.314569473\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.593144894\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t9.309650421\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 30 ------------------------------\n",
      "rewards/mean:\t1.892591476\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.853230894\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t8.995855331\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 31 ------------------------------\n",
      "rewards/mean:\t1.658497810\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.767682672\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t9.927598953\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 32 ------------------------------\n",
      "rewards/mean:\t2.242645264\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.822695374\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t11.292154312\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 33 ------------------------------\n",
      "rewards/mean:\t2.507382393\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t1.033603668\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t9.609386444\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 34 ------------------------------\n",
      "rewards/mean:\t1.669645667\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.738251328\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t9.691307068\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 35 ------------------------------\n",
      "rewards/mean:\t1.641262054\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.854317665\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t10.706752777\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 36 ------------------------------\n",
      "rewards/mean:\t2.518949509\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t1.045243740\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t10.646293640\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 37 ------------------------------\n",
      "rewards/mean:\t2.308213234\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t1.085758448\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t10.641125679\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 38 ------------------------------\n",
      "rewards/mean:\t2.757038116\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t1.259584904\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t11.754118919\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 39 ------------------------------\n",
      "rewards/mean:\t1.875798702\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t1.078908682\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t10.698418617\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 40 ------------------------------\n",
      "rewards/mean:\t2.025342941\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t1.018032551\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t10.038543701\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 41 ------------------------------\n",
      "rewards/mean:\t2.966987610\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t1.297778368\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t12.862883568\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 42 ------------------------------\n",
      "rewards/mean:\t2.380261183\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t1.269925833\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t11.148300171\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 43 ------------------------------\n",
      "rewards/mean:\t1.641786098\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t0.875130177\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t10.379568100\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 44 ------------------------------\n",
      "rewards/mean:\t2.267345428\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t1.228146553\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t10.731595993\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 45 ------------------------------\n",
      "rewards/mean:\t2.185069084\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t1.044715405\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t11.956440926\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 46 ------------------------------\n",
      "rewards/mean:\t2.742191315\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t1.486500025\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t11.958680153\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 47 ------------------------------\n",
      "rewards/mean:\t2.440894127\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t1.329435825\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t10.206062317\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 48 ------------------------------\n",
      "rewards/mean:\t3.018871307\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t1.613835812\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t11.758439064\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 49 ------------------------------\n",
      "rewards/mean:\t3.491214752\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t1.719527960\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t14.244935989\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "max_steps = 50\n",
    "generation_kwargs = dict(\n",
    "    min_length=-1, max_new_tokens=128, do_sample=True, top_k=0, top_p=1.0, pad_token_id=main_tokenizer.eos_token_id)\n",
    "\n",
    "with tqdm(enumerate(ppo_trainer.dataloader), total=max_steps) as progressbar:\n",
    "  # note: ppo_trainer.dataloader is just a regular dataloader of queries, no RL-specific magic :)\n",
    "  for epoch, batch in progressbar:\n",
    "    if epoch >= max_steps:\n",
    "        break\n",
    "\n",
    "    # Rollout stage: generate continuations from batch queries using main_model\n",
    "    response_tensors = ppo_trainer.generate(batch['input_ids'], **generation_kwargs)\n",
    "\n",
    "    # de-tokenize responses to strings (since reward model uses a different tokenizer)\n",
    "    batch[\"response\"] = [main_tokenizer.decode(response.squeeze()) for response in response_tensors]\n",
    "    # response_tensors already contain query tokens, so we don't need to add queries manually.\n",
    "\n",
    "\n",
    "    # Evaluation stage\n",
    "    rewards = compute_reward(batch['response'])\n",
    "\n",
    "    # Update stage\n",
    "    stats = ppo_trainer.step(batch['input_ids'], response_tensors, list(rewards.split(1)))\n",
    "    stats['rewards/mean'] = rewards.mean().item()\n",
    "\n",
    "    print(\"-\" * 30, 'STEP', epoch, '-' * 30)\n",
    "    print(f'rewards/mean:\\t{stats[\"rewards/mean\"]:.9f}\\t<---- average reward over this batch (higher=better, noisy)')\n",
    "    print(f'ppo/returns/mean:\\t{stats[\"ppo/returns/mean\"]:.9f}\\t<---- model-estimated average discounted reward')\n",
    "    print(f'objective/kl:\\t{stats[\"objective/kl\"]:.9f}\\t<---- how far we are from the original model (regularizer)')\n",
    "    print()\n",
    "\n",
    "    ppo_trainer.log_stats(stats, batch, list(rewards.split(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GNc4fx-rxDKv",
    "outputId": "6d2fde0d-9ba3-48d2-a761-63babe6f004f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated text: The movie has to be seen as a warning to anyone who sees this film. I don't have an issue with the content, but I think it was poorly paced and a bad idea. It's a cheap attempt at something that just goes down the drain.\n"
     ]
    }
   ],
   "source": [
    "inputs = main_tokenizer(\"The movie\", return_tensors='pt').to(device)\n",
    "generated_ids = main_model.model.generate(**inputs, max_new_tokens=50, do_sample=True)\n",
    "print(\"\\nGenerated text:\", main_tokenizer.decode(generated_ids.flatten().cpu().numpy().tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly a nagative review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hgtmjtilq6T8"
   },
   "source": [
    "## Actually training the model\n",
    "\n",
    "\n",
    "Now we use the RLHF pipeline to train a model to generate very short reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZNCrTlASL9E"
   },
   "source": [
    "### Creating reward model and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T21:54:48.895707Z",
     "iopub.status.busy": "2024-12-15T21:54:48.894601Z",
     "iopub.status.idle": "2024-12-15T21:55:13.164379Z",
     "shell.execute_reply": "2024-12-15T21:55:13.163443Z",
     "shell.execute_reply.started": "2024-12-15T21:54:48.895664Z"
    },
    "id": "uQX4Z5U_69-l",
    "outputId": "ad1e5187-77dc-473f-d6bb-5ac04babfc1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: The script transformers-cli is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts accelerate, accelerate-config, accelerate-estimate-memory and accelerate-launch are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fastai 2.7.12 requires torch<2.1,>=1.7, but you have torch 2.5.1+cu118 which is incompatible.\n",
      "torchdata 0.6.1 requires torch==2.0.1, but you have torch 2.5.1+cu118 which is incompatible.\n",
      "torchtext 0.15.2 requires torch==2.0.1, but you have torch 2.5.1+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install -q trl==0.7.4 transformers==4.33.1 accelerate==0.28.0 datasets peft==0.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wzy9RW7469-l"
   },
   "outputs": [],
   "source": [
    "%pip install -q --upgrade trl   #optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T21:59:27.808952Z",
     "iopub.status.busy": "2024-12-15T21:59:27.807999Z",
     "iopub.status.idle": "2024-12-15T21:59:30.279429Z",
     "shell.execute_reply": "2024-12-15T21:59:30.278542Z",
     "shell.execute_reply.started": "2024-12-15T21:59:27.808916Z"
    },
    "id": "Iw_c6OTz69-m",
    "outputId": "d571ed7e-0b62-4b25-9029-0f0a19403fc2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentencepiece in /home/jupyter/.local/lib/python3.10/site-packages (0.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T15:13:32.990329Z",
     "iopub.status.busy": "2024-12-16T15:13:32.989172Z",
     "iopub.status.idle": "2024-12-16T15:13:45.530756Z",
     "shell.execute_reply": "2024-12-16T15:13:45.529900Z",
     "shell.execute_reply.started": "2024-12-16T15:13:32.990284Z"
    },
    "id": "mkzjec_LJuGj",
    "outputId": "1738648f-ed29-44d8-9597-720432c13ed6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torchimport transformers\n",
    "import trl\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import sentencepiece\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"torch.utils.checkpoint:\")\n",
    "warnings.filterwarnings('ignore', message='`max_length` is ignored')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T15:13:45.533563Z",
     "iopub.status.busy": "2024-12-16T15:13:45.532234Z",
     "iopub.status.idle": "2024-12-16T15:13:49.129441Z",
     "shell.execute_reply": "2024-12-16T15:13:49.128510Z",
     "shell.execute_reply.started": "2024-12-16T15:13:45.533519Z"
    },
    "id": "a-yzbpq069-m",
    "outputId": "05a3f46b-e165-4cd2-8008-a0f367f3a2c2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:488: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.bias', 'classifier.weight', 'classifier.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "reward_model = transformers.AutoModelForSequenceClassification.from_pretrained('microsoft/deberta-v3-base', device_map=device)\n",
    "reward_tokenizer = transformers.AutoTokenizer.from_pretrained('microsoft/deberta-v3-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T15:13:55.006576Z",
     "iopub.status.busy": "2024-12-16T15:13:55.005756Z",
     "iopub.status.idle": "2024-12-16T15:13:57.995751Z",
     "shell.execute_reply": "2024-12-16T15:13:57.994884Z",
     "shell.execute_reply.started": "2024-12-16T15:13:55.006534Z"
    },
    "id": "5Im4a_--LPkm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = load_dataset('Dahoas/synthetic-instruct-gptj-pairwise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T15:13:57.998101Z",
     "iopub.status.busy": "2024-12-16T15:13:57.997118Z",
     "iopub.status.idle": "2024-12-16T15:13:58.015099Z",
     "shell.execute_reply": "2024-12-16T15:13:58.014335Z",
     "shell.execute_reply.started": "2024-12-16T15:13:57.998058Z"
    },
    "id": "usMJv-9cD2VO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_share = int(len(ds['train']) * 0.9)\n",
    "rm_train_ds = ds['train'].select(range(train_share))\n",
    "rm_test_ds = ds['train'].select(range(train_share, len(ds['train'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T14:55:47.056346Z",
     "iopub.status.busy": "2024-12-16T14:55:47.055914Z",
     "iopub.status.idle": "2024-12-16T14:55:47.081189Z",
     "shell.execute_reply": "2024-12-16T14:55:47.080406Z",
     "shell.execute_reply.started": "2024-12-16T14:55:47.056326Z"
    },
    "id": "-5SpO_A169-n",
    "outputId": "7d160760-eed1-4de0-8bfa-953d835e1b87",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29828\n",
      "3315\n"
     ]
    }
   ],
   "source": [
    "print(len(rm_train_ds))\n",
    "print(len(rm_test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-12-16T14:55:47.082272Z",
     "iopub.status.busy": "2024-12-16T14:55:47.081956Z",
     "iopub.status.idle": "2024-12-16T14:55:47.110157Z",
     "shell.execute_reply": "2024-12-16T14:55:47.109393Z",
     "shell.execute_reply.started": "2024-12-16T14:55:47.082252Z"
    },
    "id": "I2-i2OERTDH5",
    "outputId": "aa1a0d65-3726-48fa-e07b-aa4978018fea",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample prompt:\n",
      " How should I handle a disagreement with my boss.\n",
      "\n",
      "\n",
      "sample chosen:\n",
      " The best way to handle a disagreement with your boss is to remain professional and focus on finding a solution that respects both of your views. Start by articulating your opinion clearly and factually, respectfully listening to your boss's point of view, and then working together to find a mutually beneficial solution.\n",
      "\n",
      "\n",
      "sample rejected:\n",
      " You could be respectful and open, or you could give your boss a better apology.\n"
     ]
    }
   ],
   "source": [
    "print('sample prompt:\\n', rm_train_ds[13]['prompt'])\n",
    "print('\\n')\n",
    "print('sample chosen:\\n', rm_train_ds[13]['chosen'])\n",
    "print('\\n')\n",
    "print('sample rejected:\\n', rm_train_ds[13]['rejected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T14:55:47.111401Z",
     "iopub.status.busy": "2024-12-16T14:55:47.110910Z",
     "iopub.status.idle": "2024-12-16T14:55:47.175145Z",
     "shell.execute_reply": "2024-12-16T14:55:47.174403Z",
     "shell.execute_reply.started": "2024-12-16T14:55:47.111380Z"
    },
    "id": "4AsdS6xo69-o",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    chosen = reward_tokenizer(examples[\"prompt\"] + '\\n' + examples[\"chosen\"], truncation=True, max_length=512)   # conditioning on prompt\n",
    "    rejected = reward_tokenizer(examples[\"prompt\"] + '\\n' + examples[\"rejected\"], truncation=True, max_length=512)\n",
    "    return {\n",
    "        \"input_ids_chosen\": chosen[\"input_ids\"],\n",
    "        \"attention_mask_chosen\": chosen[\"attention_mask\"],\n",
    "        \"input_ids_rejected\": rejected[\"input_ids\"],\n",
    "        \"attention_mask_rejected\": rejected[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "rm_train_dict_ds = rm_train_ds.map(preprocess_function, batched=False)\n",
    "rm_test_dict_ds = rm_test_ds.map(preprocess_function, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 923,
     "referenced_widgets": [
      "b379e83fdc8a4ed68dd392545034f3ba",
      "fd0cb88718174527a705bf87736e1b33",
      "e0390f14e3ef411699b3263cad80f564",
      "82986f9466a9476d9a199d9d78b7f56b",
      "4780e68b1b3c4104ba68ea689f22f047",
      "f97ef1004924404ebd590606159ca82d",
      "5d8e9471490b45eea5921bce6df8cf59",
      "5da5d5133f6d4275b87f91441554069c",
      "ae3d59a449b4429ba028a6e5abf4e785",
      "87f8c76db8714594a96cdc1bbd3c0b0c",
      "2a341a521f98467990cc54448146064f",
      "a6c974db20c749a8bf355df6738ad0d7",
      "416d3b878a1048bf9984269e0e6af528",
      "b001eaa5cd30471f8333466283f569b9",
      "8e80e34b8ce2458da919fedb3f2d2f7d",
      "216296feff394673b92603c8ebd0bb65",
      "c006296d61c4461489f05a93a805fdc7",
      "531b41d3dc574438bbc3e885c304c0d1",
      "ec42e9face854ce98e75cce9b3360401",
      "768311637ac34db698e146de88e570d5",
      "a7b421c3cef44b65be58945749a79665",
      "0eca6b809ab64573b7a5516dfb67924b",
      "71907333ca8a4c52b3f802fdfdcec2e5",
      "8d19a441fea847b0a3fdb20ae3935caf",
      "5cf8250e9e9a4350b1c233dc53cf68e1",
      "75f19909b60b4243b6eccd8bf82333c3",
      "81013f60bc204194b2ab420095ec805f",
      "d1d108ae1a6a4dc2bfb381c72115fa0d",
      "ca21e655dff844aabd5fd2caf602c385",
      "ae3c574930014097acdeb6aa84ea127e",
      "8558b22e3dad4539a42c219f702e5a3a",
      "fd99cbd99fbf4c9ebaa190d0bcc13979",
      "4f5aaecf1c09448faab60a2747033976"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-12-16T14:55:47.340376Z",
     "iopub.status.busy": "2024-12-16T14:55:47.339513Z",
     "iopub.status.idle": "2024-12-16T15:04:10.641303Z",
     "shell.execute_reply": "2024-12-16T15:04:10.640311Z",
     "shell.execute_reply.started": "2024-12-16T14:55:47.340354Z"
    },
    "id": "mIJKayOvKsYI",
    "outputId": "e0cc5402-ba4c-4cf9-a5cc-ecadb2e5ad34",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3377, 'learning_rate': 1.3395e-05, 'epoch': 0.05}\n",
      "{'loss': 0.0276, 'learning_rate': 1.269e-05, 'epoch': 0.11}\n",
      "{'loss': 0.0124, 'learning_rate': 1.1985e-05, 'epoch': 0.16}\n",
      "{'loss': 0.0085, 'learning_rate': 1.128e-05, 'epoch': 0.21}\n",
      "{'loss': 0.0051, 'learning_rate': 1.0575e-05, 'epoch': 0.27}\n",
      "{'loss': 0.005, 'learning_rate': 9.87e-06, 'epoch': 0.32}\n",
      "{'loss': 0.0049, 'learning_rate': 9.165000000000001e-06, 'epoch': 0.38}\n",
      "{'loss': 0.0046, 'learning_rate': 8.46e-06, 'epoch': 0.43}\n",
      "{'loss': 0.0035, 'learning_rate': 7.755000000000001e-06, 'epoch': 0.48}\n",
      "{'loss': 0.0045, 'learning_rate': 7.05e-06, 'epoch': 0.54}\n",
      "{'loss': 0.0023, 'learning_rate': 6.345e-06, 'epoch': 0.59}\n",
      "{'loss': 0.0027, 'learning_rate': 5.64e-06, 'epoch': 0.64}\n",
      "{'loss': 0.0032, 'learning_rate': 4.935e-06, 'epoch': 0.7}\n",
      "{'loss': 0.0021, 'learning_rate': 4.23e-06, 'epoch': 0.75}\n",
      "{'loss': 0.0029, 'learning_rate': 3.525e-06, 'epoch': 0.8}\n",
      "{'loss': 0.0022, 'learning_rate': 2.82e-06, 'epoch': 0.86}\n",
      "{'loss': 0.002, 'learning_rate': 2.115e-06, 'epoch': 0.91}\n",
      "{'loss': 0.0043, 'learning_rate': 1.41e-06, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0008, 'learning_rate': 7.05e-07, 'epoch': 1.02}\n",
      "{'loss': 0.0005, 'learning_rate': 0.0, 'epoch': 1.07}\n",
      "{'train_runtime': 475.6744, 'train_samples_per_second': 67.273, 'train_steps_per_second': 2.102, 'train_loss': 0.021835564147680998, 'epoch': 1.07}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=0.021835564147680998, metrics={'train_runtime': 475.6744, 'train_samples_per_second': 67.273, 'train_steps_per_second': 2.102, 'train_loss': 0.021835564147680998, 'epoch': 1.07})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = trl.RewardConfig(  # like transformers.TrainingArguments\n",
    "    output_dir=\"reward_model\",\n",
    "    max_length=512,\n",
    "    disable_tqdm=True,\n",
    "    remove_unused_columns=False,\n",
    "    per_device_train_batch_size=32,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=1.41e-5,\n",
    "    max_steps=1000,              # note: training may need more than 1k steps\n",
    "    logging_steps=50,\n",
    "    gradient_checkpointing=True,  # reduce memory usage but train ~30% slower\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    fp16=True,\n",
    "    dataloader_num_workers=os.cpu_count(),\n",
    ")\n",
    "\n",
    "trainer = trl.RewardTrainer(\n",
    "    model=reward_model,\n",
    "    args=training_args,\n",
    "    tokenizer=reward_tokenizer,\n",
    "    train_dataset=rm_train_dict_ds,\n",
    "    peft_config=None,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-12-16T15:04:10.663445Z",
     "iopub.status.busy": "2024-12-16T15:04:10.663097Z",
     "iopub.status.idle": "2024-12-16T15:04:10.683711Z",
     "shell.execute_reply": "2024-12-16T15:04:10.682835Z",
     "shell.execute_reply.started": "2024-12-16T15:04:10.663422Z"
    },
    "id": "5aNWTq7YKv7x",
    "outputId": "1585fb90-2b01-4713-e18a-0665cb50cc15"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebertaV2ForSequenceClassification(\n",
       "  (deberta): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): StableDropout()\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (pooler): ContextPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): StableDropout()\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): StableDropout()\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_model.gradient_checkpointing_disable()\n",
    "reward_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T15:04:10.685744Z",
     "iopub.status.busy": "2024-12-16T15:04:10.684933Z",
     "iopub.status.idle": "2024-12-16T15:04:14.903727Z",
     "shell.execute_reply": "2024-12-16T15:04:14.902840Z",
     "shell.execute_reply.started": "2024-12-16T15:04:10.685720Z"
    },
    "id": "239OZsH0Kzrb"
   },
   "outputs": [],
   "source": [
    "torch.save(reward_model.state_dict(), 'my_reward_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WDjItreK7oQ"
   },
   "source": [
    "SANITY CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T15:14:18.265783Z",
     "iopub.status.busy": "2024-12-16T15:14:18.264776Z",
     "iopub.status.idle": "2024-12-16T15:14:18.280348Z",
     "shell.execute_reply": "2024-12-16T15:14:18.279571Z",
     "shell.execute_reply.started": "2024-12-16T15:14:18.265737Z"
    },
    "id": "VwS2nNP0j_x6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T15:15:15.533352Z",
     "iopub.status.busy": "2024-12-16T15:15:15.532258Z",
     "iopub.status.idle": "2024-12-16T15:15:19.641645Z",
     "shell.execute_reply": "2024-12-16T15:15:19.640854Z",
     "shell.execute_reply.started": "2024-12-16T15:15:15.533305Z"
    },
    "id": "MCd-c_Qq69-p",
    "outputId": "6299fd44-b3bb-4340-f1e7-d81ec59240fc",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.bias', 'classifier.weight', 'classifier.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_2259/1973883036.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  reward_model.load_state_dict(torch.load('my_reward_model.pt', map_location=device))\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "reward_model = transformers.AutoModelForSequenceClassification.from_pretrained('microsoft/deberta-v3-base', device_map=device)\n",
    "reward_model.load_state_dict(torch.load('my_reward_model.pt', map_location=device))\n",
    "reward_tokenizer = transformers.AutoTokenizer.from_pretrained('microsoft/deberta-v3-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T15:15:26.142912Z",
     "iopub.status.busy": "2024-12-16T15:15:26.141695Z",
     "iopub.status.idle": "2024-12-16T15:15:26.181436Z",
     "shell.execute_reply": "2024-12-16T15:15:26.180496Z",
     "shell.execute_reply.started": "2024-12-16T15:15:26.142868Z"
    },
    "id": "eA78BHSC69-q",
    "outputId": "b2428bdb-4a0f-41ad-b97a-a69338f93494"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebertaV2ForSequenceClassification(\n",
       "  (deberta): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): StableDropout()\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (pooler): ContextPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): StableDropout()\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): StableDropout()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_model.gradient_checkpointing_disable()\n",
    "reward_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "72d84000f4be48ee914c41b2c665d46b",
      "0efbfcab34104217ada6d872b91685e6",
      "80421bc7d14f4e91be0089cc38c65ef1",
      "7fb0efab75b646deb8074191542eebdc",
      "4c0b47c15ceb4016aacb375e999959a7",
      "8e6f3743c8aa4cd5b5362d2668996a82",
      "bd364c597b3c42ea93577da333bb7d63",
      "e6cb78bcf33945299b821ee33a88c7a7",
      "1fdadbaed9134981be15091286e43b9d",
      "83082b60a7914054bf7b1aaf34ec3842",
      "5abc364082e34f6688248bdcda6b7b5c"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-12-16T15:19:27.977761Z",
     "iopub.status.busy": "2024-12-16T15:19:27.976660Z",
     "iopub.status.idle": "2024-12-16T15:33:47.908133Z",
     "shell.execute_reply": "2024-12-16T15:33:47.907164Z",
     "shell.execute_reply.started": "2024-12-16T15:19:27.977731Z"
    },
    "id": "HgCx02uekFjk",
    "outputId": "d7b87ac6-3471-41ef-c7e5-c11b9c0a5b25",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3315/3315 [14:19<00:00,  3.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy of reward model:  0.9993966817496229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correct_comparisons = 0\n",
    "for examples in tqdm(rm_test_ds):\n",
    "    chosen = reward_tokenizer(examples[\"prompt\"] + '\\n' + examples[\"chosen\"], truncation=True, return_tensors='pt', max_length=512).to(device)   # conditioning on prompt\n",
    "    rejected = reward_tokenizer(examples[\"prompt\"] + '\\n' + examples[\"rejected\"], truncation=True, return_tensors='pt', max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        reward_chosen = reward_model(**chosen).logits[0, 0].item()\n",
    "        reward_rejected = reward_model(**rejected).logits[0, 0].item()\n",
    "    if reward_chosen > reward_rejected:\n",
    "        correct_comparisons += 1\n",
    "print('Test accuracy of reward model: ', correct_comparisons / len(rm_test_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "260ArFr6oimw"
   },
   "source": [
    "## Part 2. RL fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQ3fsKD-qv7k"
   },
   "source": [
    "I'll try to make gpt2-large write shorter responses. I'll use **negative** response length as a reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1cxUW-Ki8QUY",
    "outputId": "3b9402a8-98ef-438b-ec7d-1a80ea3d7d94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.9/119.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.9/133.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.6/85.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.1/112.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\n",
      "sentence-transformers 3.2.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.33.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%pip install -q trl==0.7.4 transformers==4.33.1 accelerate==0.28.0 datasets peft==0.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-12-16T23:26:26.213202Z",
     "iopub.status.busy": "2024-12-16T23:26:26.212100Z",
     "iopub.status.idle": "2024-12-16T23:27:04.900565Z",
     "shell.execute_reply": "2024-12-16T23:27:04.899710Z",
     "shell.execute_reply.started": "2024-12-16T23:26:26.213156Z"
    },
    "id": "nJENXeMN69-r",
    "outputId": "d7e5b6b4-2f09-4d27-e90e-8db08fa020f4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM\n",
    "import trl\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import sentencepiece\n",
    "import warnings\n",
    "import os\n",
    "from typing import List\n",
    "import peft\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"torch.utils.checkpoint:\")\n",
    "warnings.filterwarnings('ignore', message='`max_length` is ignored')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435,
     "referenced_widgets": [
      "c46e629f6bb24a4ca5bdc899d5f8e00c",
      "2df72c58f63c4128ac44d6fa06c32940",
      "c4a063bcc5464289ab0cd826b1c36860",
      "9cf7fe6b79b24bd1bbb562d6db374d2b",
      "1a70b168891e47ab986c27f559912ddc",
      "44fd7eb87ff04a12aaa234038f46ed77",
      "a4f5dbc03457435d9ca45f1bb6dc64cc",
      "95a83cae1ed84af7bc87d9c9faa6d4e9",
      "b9bce7a5b60943fba29b5547f3e25926",
      "cb18426df2d643169c715c4645302800",
      "5514fb136cc2404e8fb6b584d3c4d6aa",
      "5268f97f9a9b472096c2a7780d5d1931",
      "ad17ec9a2e694ea9a20e8c207aec5a70",
      "3562e9a00c8a4fdbae15344c39a3253c",
      "13f358eea3f340208b5b50a08dbb357b",
      "a1efbcaa68e541aaada27b874ecceb32",
      "6012bb74d9d24e5e8155f4f68efd6e5c",
      "da4475a7ccea4574800e4caecbe8e882",
      "1931cd0c7e444199bea1624a2ee7d13c",
      "cba211e6ef2044ae9a94af8a4d64354d",
      "92e1936f1f194c2e9834e78cfabfaf1f",
      "7af5523b78404ceeb66bc32e9fc83101",
      "3041072fbfa340c6a1f260318855ffcf",
      "914cc004c353436fbd947e15e41b8413",
      "35b342facb054448b142425e4a6fca26",
      "5492c7d718de437688d55ebefc67328e",
      "96415151d73640a4adc9d0cb3f5fe19d",
      "7de06a212cf84cfcad61f654ee9f93e3",
      "fcbc0b58bf094b67904f6a4b5f158baf",
      "56aace1588b14ed0973f0b94730401e9",
      "4a4922c2a6cb4e1bb306afb366b5b0c7",
      "127f04e413b94939b634003b11160150",
      "9b8ab7c55ff5451aa79b046f4ffc67e4",
      "ad66a15df54f4cc1a372442ff9619a5c",
      "3d90baa9bf5847cbbf82809809f68a61",
      "d1b990930a5d437ea8347456792e4703",
      "961d236138e44a93a7b7c768a8e5303f",
      "7da822e17f4c4c918c39fb9180661cf1",
      "5fbc256f84f1442ea9545e293cf64a79",
      "2ef4e2e00f944dc488123d317364ea25",
      "f7d313d494654f2eb23ed6ebdeccd14d",
      "28d09aa5a7ce4ca384a9fb0279cba6d3",
      "01c45ed91e5f4f558dd5c8dbfa6b7ea5",
      "5c14eac3d8d64df995acbbe90ab259be",
      "3e7d510ca49b4e87aa0cfc9705324a7d",
      "7b9f1154233047e4a4d9e0b480248dc1",
      "7d854703eb8b4dcca8b6bbd535a478ca",
      "1179110bf2384e7688f3ac447149cf6c",
      "190a9c43d038420885714212235dda0f",
      "b24d0180b713449bad1242cdfec4128b",
      "e59fece44804429b9d033c225a9dcb08",
      "402ff24f06f74943840bdcf5a3e5e918",
      "3b61d67fab80400e98810470696387eb",
      "e359d6bbc23f4fc39833a5eec3b1b534",
      "0c202b540ae84f45aa763b52a67ef58a",
      "a86f0e6a05df4517af69c43ca7115ff5",
      "14be3b1b5cb84352b162ab5a307bd080",
      "3d7a97c476074c5a9609ffd43659b096",
      "acf694b0112a4ea3b7c8d05e8e598605",
      "fad5c12b43f641d0b9b54fecbd4b9a38",
      "2f159bca51ec43158c6731026d576db2",
      "be9988d178254e3abf32953d000203b5",
      "589ec75143f1433abd1a0211df079729",
      "38c1ff9f01d94967bcba7b8a883b0865",
      "5f3848bbdf4a4227ba4fd6f881a6eb3d",
      "548024f72a5847fd9d60b9fff49c058e",
      "ea56937d9ba0449e83533c3649263107",
      "59e3c43c0ac64f098034245dd3161a80",
      "d96c9f9ec8b34b1bb215280374f61a19",
      "33f5d601df874237883d0ad2b081aff6",
      "7e42e3284d694b84a4fe94b8690183a9",
      "40d569b9ffe049b7a04a3e6c18eedc87",
      "c05e9346ff474a22b28d1de00d4803ad",
      "1c32f135b53c42a4abddc554b66a84a3",
      "1c625c65ed5b40c1824027dfd20f31f7",
      "3a51d1efc0d1477bae6386e5143880a4",
      "e26de089d18a4131b1b04228288f6416"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-12-16T23:27:13.778057Z",
     "iopub.status.busy": "2024-12-16T23:27:13.777310Z",
     "iopub.status.idle": "2024-12-16T23:27:38.583827Z",
     "shell.execute_reply": "2024-12-16T23:27:38.582822Z",
     "shell.execute_reply.started": "2024-12-16T23:27:13.778019Z"
    },
    "id": "mjTN3VU869-s",
    "outputId": "2730398c-fdda-41eb-e27d-0dc740a92146",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c46e629f6bb24a4ca5bdc899d5f8e00c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5268f97f9a9b472096c2a7780d5d1931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3041072fbfa340c6a1f260318855ffcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad66a15df54f4cc1a372442ff9619a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e7d510ca49b4e87aa0cfc9705324a7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a86f0e6a05df4517af69c43ca7115ff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.25G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea56937d9ba0449e83533c3649263107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main_tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2-large\")\n",
    "main_tokenizer.pad_token_id = main_tokenizer.eos_token_id\n",
    "main_tokenizer.padding_side='left'\n",
    "\n",
    "main_model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2-large\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VX5W_Ny7n9xZ"
   },
   "source": [
    "Creating a dataset with short prompts (3 tokens each)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "4436364429ea431594dda34d6fbd21b5",
      "65d028d9746f42e98312eefb2f954edc",
      "4546cbcec6b14ca595f04f1189172236",
      "acfcf22b62274b5286c1bf407c23b7c0",
      "e642ced0bdcc4372aeb56d3d8a2ef279",
      "2116f80d08894f3fbaf320d45fb1b4ce",
      "4982dd5bbe5b47bbaeebbaa3fd50940b",
      "13288ce63dac4b8eadd0a5da787902ac",
      "937597205e7b4da587916d5201908e8b",
      "ad6eb58998b24b02ba52c89f7e65539d",
      "f798d46337d2492e8fcedbe82a1bfeb8",
      "e6b84a5e4f9f4bc09af2c5bacb126b00",
      "bb04d1df97ae4c80b0c58b46e6b4f4cb",
      "63f90810552b41e882da8a208fec21a2",
      "5a2ea94e99d743cabb091b9480e190fb",
      "3cb84febbde54873b1c8736d01558ef1",
      "85d9f1cdf29c4958ace617c1e88a9fe8",
      "0448a4f8ea484ab2af8c37c1e5460fa4",
      "16e55a1176b242b6bf8e7ddb1a91753f",
      "7863ece91e2243a1a69e48ea84a929ad",
      "a40b655a52884bd39b36ef27a94e2acc",
      "37486e6393464d72b3d0e775672439b4",
      "cd95b63ed1804149bdbbde1f417ef827",
      "984405b6f0d9415cb876bef709128bed",
      "f518869820724d1da76edbe7a936b2a8",
      "b76f4665e2ea4c278f46765c9c7ccd02",
      "a15f5c11f04b4b74a0cbf6455bbf19b0",
      "89d4d6e3d9ba4a7a8fddf64f37b618b3",
      "7caa70f8ba214bc7965f22500bb01399",
      "e7c109b0f8e4499f8389cbb68174f4b2",
      "ff33d915d3d643049a5d5fce3712870f",
      "6b89b6ca103a45dca4f9dc4d56ebeda9",
      "3e1b232826c54bb7a8c1d6a1b0e721d0",
      "7270231e32084614b6f664377ba4956e",
      "811d5d02566141b6aafb4b2d1109c483",
      "e93e1d5f5abd4521b36d5f58598a0d96",
      "d6c43d9fe468420a83b5c367fa499fde",
      "fdeb2efe4ff749078492be8f480787fa",
      "7e175a7758494f09bea6c8b239c7059b",
      "1e24cd5e72fb4010a09573e0590ea688",
      "6ba92a35387f45fc914cea3a7ea2b485",
      "daf150126a544d9da92d000430be40f4",
      "b82e0fcfc5a34a9b8d9b0756f41acecb",
      "e16b99f264b34c6bb0f2e2705a629fad"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-12-16T23:27:38.587652Z",
     "iopub.status.busy": "2024-12-16T23:27:38.585006Z",
     "iopub.status.idle": "2024-12-16T23:27:45.804663Z",
     "shell.execute_reply": "2024-12-16T23:27:45.803884Z",
     "shell.execute_reply.started": "2024-12-16T23:27:38.587611Z"
    },
    "id": "CFg9oABB69-s",
    "outputId": "75334565-e1c6-476a-b87a-60f17f7f6c8a",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4436364429ea431594dda34d6fbd21b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/951 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6b84a5e4f9f4bc09af2c5bacb126b00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openwebtext-10k.py:   0%|          | 0.00/3.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd95b63ed1804149bdbbde1f417ef827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0000.parquet:   0%|          | 0.00/30.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7270231e32084614b6f664377ba4956e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_base = load_dataset(\"stas/openwebtext-10k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86,
     "referenced_widgets": [
      "f4e32623f02b432fa4010fdb00c3bb3c",
      "d850f7a0ab7d406ab2777770ab264e63",
      "0672136c903b4a83a42430c2f7aa3257",
      "de48c7a6852a40cf8bd53379edb50a87",
      "2752c4442c9649e887b981ad5e954bca",
      "6eb828550a9143fa977e1b3e030d6d71",
      "02a5905069ce4c38a9cbb30cc7b33e6b",
      "7bd305fcebf948f587bc4551c296697b",
      "01fa4e8b82e5403e94131073fbdbcc91",
      "6d5b6eafa3eb45abaa22db1cffa20763",
      "a604af6b9e484bdeba7344ea69fa9a33"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-12-16T23:27:45.806303Z",
     "iopub.status.busy": "2024-12-16T23:27:45.805638Z",
     "iopub.status.idle": "2024-12-16T23:27:45.916696Z",
     "shell.execute_reply": "2024-12-16T23:27:45.915855Z",
     "shell.execute_reply.started": "2024-12-16T23:27:45.806267Z"
    },
    "id": "b5oeSKRvko1Q",
    "outputId": "5231f0ce-92ef-4c13-e760-aa9440af3484",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4e32623f02b432fa4010fdb00c3bb3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1094 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "def select_query_and_tokenize(sample):\n",
    "    query_ids = main_tokenizer.encode(sample['text'])[: 3]\n",
    "    sample[\"input_ids\"] = query_ids\n",
    "    sample[\"query\"] = main_tokenizer.decode(query_ids)\n",
    "    return sample\n",
    "\n",
    "ds = ds_base.map(select_query_and_tokenize, batched=False)\n",
    "ds.set_format(type=\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T23:27:45.918616Z",
     "iopub.status.busy": "2024-12-16T23:27:45.917669Z",
     "iopub.status.idle": "2024-12-16T23:27:49.004021Z",
     "shell.execute_reply": "2024-12-16T23:27:49.003111Z",
     "shell.execute_reply.started": "2024-12-16T23:27:45.918576Z"
    },
    "id": "pvSBBe9q69-t",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_train = ds['train'][:9500]\n",
    "ds_test = ds['train'][9500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5rMxNWOFC7d"
   },
   "source": [
    "RLDataset for training with reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T23:27:49.005730Z",
     "iopub.status.busy": "2024-12-16T23:27:49.005194Z",
     "iopub.status.idle": "2024-12-16T23:27:49.041962Z",
     "shell.execute_reply": "2024-12-16T23:27:49.041094Z",
     "shell.execute_reply.started": "2024-12-16T23:27:49.005695Z"
    },
    "id": "6IsdEZij69-t",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RLDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, d):\n",
    "        self.queries = d['query']\n",
    "        self.input_ids = d['input_ids']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.queries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'query': self.queries[idx],\n",
    "            'input_ids': self.input_ids[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T23:27:49.043773Z",
     "iopub.status.busy": "2024-12-16T23:27:49.042914Z",
     "iopub.status.idle": "2024-12-16T23:27:49.060802Z",
     "shell.execute_reply": "2024-12-16T23:27:49.059924Z",
     "shell.execute_reply.started": "2024-12-16T23:27:49.043735Z"
    },
    "id": "qHIVS3lv69-t",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_train = RLDataset(ds_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBzc0hkKFO4i"
   },
   "source": [
    "Function for calculating the reward - the negative length of the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T23:30:32.665615Z",
     "iopub.status.busy": "2024-12-16T23:30:32.662772Z",
     "iopub.status.idle": "2024-12-16T23:30:32.691680Z",
     "shell.execute_reply": "2024-12-16T23:30:32.690637Z",
     "shell.execute_reply.started": "2024-12-16T23:30:32.665573Z"
    },
    "id": "tAqh_LcW69-v",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_reward(texts: List[str]) -> torch.Tensor:\n",
    "    return torch.tensor([-len(text.split()) for text in texts], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-12-16T23:30:32.696903Z",
     "iopub.status.busy": "2024-12-16T23:30:32.692870Z",
     "iopub.status.idle": "2024-12-16T23:30:34.005516Z",
     "shell.execute_reply": "2024-12-16T23:30:34.004608Z",
     "shell.execute_reply.started": "2024-12-16T23:30:32.696855Z"
    },
    "id": "IMnhdTET69-v",
    "outputId": "b8479ba8-37f1-4b0e-b89d-30967d349cc0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompts: ['cute little bunny', 'jumps over a very high fence']\n",
      "Rewards: [-3, -6]\n"
     ]
    }
   ],
   "source": [
    "prompts = ['cute little bunny', 'jumps over a very high fence']\n",
    "print('Prompts:', prompts)\n",
    "print('Rewards:', compute_reward(prompts).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qR-ciOf6FW9K"
   },
   "source": [
    "Let's look at the initial distribution of the lengths of the generated texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "dfe7bfd05e744439b8c330f6e60f0b3b",
      "1932f647dfec4e469e64053f6b557873",
      "1636dc2c1ac44c1fa90d13a884576df3",
      "c2c7e2eee6154da0a89d2264fe883ae9",
      "0af6f923917346a192b3d8d59e9cc50b",
      "220231baf55c4dc5a3ee7ca07c36ce61",
      "250a0a9fae2c41dfbbc3cd001492768f",
      "27700135e2c8481da709d82dceff4104",
      "f01180605d744384873a430984b6167f",
      "1df21822e38e49d6ba3356795982b9a1",
      "646b688143a84761971252ac3c70b5ef"
     ]
    },
    "id": "v8fv_Yw0w3UA",
    "outputId": "674c1ef7-f741-4e84-8af8-77f6468aaaf3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfe7bfd05e744439b8c330f6e60f0b3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "non_rlhf_generation_lens = []\n",
    "batch_size=32\n",
    "\n",
    "for i in tqdm(range(0, len(ds_test['query']), batch_size)):\n",
    "    batch = ds_test['query'][i:i+batch_size]\n",
    "    inputs = main_tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = main_model.generate(\n",
    "            **inputs,\n",
    "            min_length=-1,\n",
    "            max_length=20,\n",
    "            do_sample=False,\n",
    "            num_beams=1,\n",
    "            temperature=1.0,\n",
    "            pad_token_id=main_tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    decoded_outputs = main_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    non_rlhf_generation_lens.extend([len(text.split()) for text in decoded_outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "execution": {
     "iopub.execute_input": "2024-12-16T23:32:13.087650Z",
     "iopub.status.busy": "2024-12-16T23:32:13.086418Z",
     "iopub.status.idle": "2024-12-16T23:32:13.102588Z",
     "shell.execute_reply": "2024-12-16T23:32:13.101666Z",
     "shell.execute_reply.started": "2024-12-16T23:32:13.087604Z"
    },
    "id": "9wNXSFWa69-w",
    "outputId": "31d6371f-d71e-4514-df25-ed28d9bf7618",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(non_rlhf_generation_lens, bins=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lengths are limited to 20 by the generation parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156,
     "referenced_widgets": [
      "f48cd04fddb04c7bb5ef6d0040dc8924",
      "956c4b1fde9e4e25b89487633cc05511",
      "c9ad1afdd4ec4d599fa8d6eccf10e692",
      "0bcd1f8c9c44409a8d23fb739e59f7a6",
      "24a30d4dad38481391a918cc6a649f64",
      "95ca0bef5d9949ed89a754517349f288",
      "82b70300e7214367ac57393caa076dd4",
      "4cf78ed0fb2e40a88c9a57b46de337dc",
      "ea845a4a2208425da3686d3135eb992e",
      "db19c329fbaa43a4a6edf13447c79996",
      "21b41a249b88497285466f088ec4de60"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-12-16T23:32:20.128003Z",
     "iopub.status.busy": "2024-12-16T23:32:20.126836Z",
     "iopub.status.idle": "2024-12-16T23:33:31.215854Z",
     "shell.execute_reply": "2024-12-16T23:33:31.214933Z",
     "shell.execute_reply.started": "2024-12-16T23:32:20.127959Z"
    },
    "id": "OzzPZbdG69-x",
    "outputId": "b611ba22-c40a-4ed4-fb61-eb1813fb060d",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f48cd04fddb04c7bb5ef6d0040dc8924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/3.25G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/trl/models/modeling_base.py:298: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = loading_func(filename if not use_safe else safe_filename, **load_kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/peft/tuners/lora.py:475: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 5,898,240 || all params: 779,929,601 || trainable%: 0.7562528710844506\n"
     ]
    }
   ],
   "source": [
    "peft_config = peft.LoraConfig(\n",
    "    task_type=peft.TaskType.CAUSAL_LM, r=32, lora_alpha=32, lora_dropout=0.0, inference_mode=False\n",
    ")\n",
    "\n",
    "main_model = trl.AutoModelForCausalLMWithValueHead.from_pretrained(\"openai-community/gpt2-large\", device_map=device)\n",
    "main_model = peft.get_peft_model(main_model, peft_config, adapter_name='default')\n",
    "main_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "3iPZ17aaHdrN"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T23:51:06.274995Z",
     "iopub.status.busy": "2024-12-16T23:51:06.273740Z",
     "iopub.status.idle": "2024-12-16T23:51:06.436235Z",
     "shell.execute_reply": "2024-12-16T23:51:06.435387Z",
     "shell.execute_reply.started": "2024-12-16T23:51:06.274932Z"
    },
    "id": "NovdWom169-x",
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = trl.PPOConfig(\n",
    "    model_name=main_model.config._name_or_path,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=1.41e-5,\n",
    "    batch_size=32,\n",
    "    ppo_epochs=4,                 # PPO performs this many updates per training batch\n",
    "    adap_kl_ctrl=True,\n",
    ")\n",
    "\n",
    "ppo_trainer = trl.PPOTrainer(\n",
    "    training_args, model=main_model.model, tokenizer=main_tokenizer,\n",
    "    dataset=ds_train, data_collator=lambda data: dict((key, [d[key] for d in data]) for key in data[0])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeQuRlUuEYQ1"
   },
   "source": [
    "Here, for the purity of the experiment, it is important that the generation parameters are the same as before - when we calculated distribution of the lengths of the generated texts before RLHF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "0d59cd7ddf9246ba85bc60e00bfeac2a",
      "9edd6e916bd3461ca350341f2af6319f",
      "7d48722a9075403090df798185e5d514",
      "b312af5e631042e68e9a725b6a372aaf",
      "6afff2beef1d449d9feeaa990e67b9b9",
      "fb7f7346b6aa4ce8a81209f0f8780094",
      "5e6bea496a134407a4474244d8454b2a",
      "f69ef82ed36d4afc8194fdf8812a3cf6",
      "43a4a7d3fe3543d4a8bb3086b78e9937",
      "7551973fd3014265bfb99414e1446bc4",
      "1929297de65d493db41d5358674a71d4"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-12-16T23:51:06.561850Z",
     "iopub.status.busy": "2024-12-16T23:51:06.560403Z"
    },
    "id": "5xxUoYsR69-x",
    "outputId": "4ac2dec3-9b72-4904-8f67-a1bf9e447aa8",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d59cd7ddf9246ba85bc60e00bfeac2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 0 ------------------------------\n",
      "rewards/mean:\t-14.718750000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-9.573780060\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.000000000\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 1 ------------------------------\n",
      "rewards/mean:\t-14.625000000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-9.635137558\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.389716089\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 2 ------------------------------\n",
      "rewards/mean:\t-15.250000000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-10.156545639\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.644772589\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 3 ------------------------------\n",
      "rewards/mean:\t-15.968750000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-10.826311111\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t1.919888496\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 4 ------------------------------\n",
      "rewards/mean:\t-15.718750000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-10.814828873\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t2.392678022\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 5 ------------------------------\n",
      "rewards/mean:\t-15.687500000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-10.857444763\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t1.346082449\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 6 ------------------------------\n",
      "rewards/mean:\t-14.218750000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-10.320177078\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t3.810068846\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 7 ------------------------------\n",
      "rewards/mean:\t-16.281250000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-11.625429153\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t3.028374195\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 8 ------------------------------\n",
      "rewards/mean:\t-13.750000000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-10.207860947\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t3.217613220\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 9 ------------------------------\n",
      "rewards/mean:\t-13.437500000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-10.247129440\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t4.139433861\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 10 ------------------------------\n",
      "rewards/mean:\t-13.500000000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-10.429330826\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t3.094831228\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 11 ------------------------------\n",
      "rewards/mean:\t-13.343750000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-10.365600586\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t3.522689104\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 12 ------------------------------\n",
      "rewards/mean:\t-12.656250000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-10.331935883\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t6.466574669\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 13 ------------------------------\n",
      "rewards/mean:\t-11.187500000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-9.477537155\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t4.440541744\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 14 ------------------------------\n",
      "rewards/mean:\t-11.031250000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-9.774040222\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t8.762428284\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 15 ------------------------------\n",
      "rewards/mean:\t-10.968750000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-9.998584747\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t8.097695351\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 16 ------------------------------\n",
      "rewards/mean:\t-9.375000000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-8.943759918\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t8.799486160\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 17 ------------------------------\n",
      "rewards/mean:\t-10.625000000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-9.711508751\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t6.963077545\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 18 ------------------------------\n",
      "rewards/mean:\t-10.281250000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-10.061238289\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t11.018299103\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 19 ------------------------------\n",
      "rewards/mean:\t-9.343750000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-9.417400360\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t11.591785431\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 20 ------------------------------\n",
      "rewards/mean:\t-8.531250000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-9.011213303\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t10.950835228\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 21 ------------------------------\n",
      "rewards/mean:\t-9.750000000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-9.927800179\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t11.148244858\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 22 ------------------------------\n",
      "rewards/mean:\t-9.906250000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-10.502636909\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t12.812953949\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 23 ------------------------------\n",
      "rewards/mean:\t-8.812500000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-9.456748009\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t12.088743210\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 24 ------------------------------\n",
      "rewards/mean:\t-9.031250000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-10.039920807\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t12.221271515\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 25 ------------------------------\n",
      "rewards/mean:\t-7.812500000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-8.866460800\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t12.605665207\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 26 ------------------------------\n",
      "rewards/mean:\t-8.593750000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-9.712879181\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t11.383640289\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 27 ------------------------------\n",
      "rewards/mean:\t-8.968750000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-9.933859825\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t10.865500450\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 28 ------------------------------\n",
      "rewards/mean:\t-8.281250000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-10.027595520\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t11.856208801\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 29 ------------------------------\n",
      "rewards/mean:\t-8.281250000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-9.815198898\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t13.331098557\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 30 ------------------------------\n",
      "rewards/mean:\t-7.156250000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-8.815477371\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t12.048198700\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 31 ------------------------------\n",
      "rewards/mean:\t-7.781250000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-9.168739319\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t12.438268661\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 32 ------------------------------\n",
      "rewards/mean:\t-7.000000000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-8.802927017\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t12.968203545\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 33 ------------------------------\n",
      "rewards/mean:\t-7.500000000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-9.200388908\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t13.697900772\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 34 ------------------------------\n",
      "rewards/mean:\t-6.812500000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-8.276932716\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t12.502279282\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 35 ------------------------------\n",
      "rewards/mean:\t-6.875000000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-8.320721626\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t11.065330505\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 36 ------------------------------\n",
      "rewards/mean:\t-7.062500000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-8.671731949\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t12.432155609\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 37 ------------------------------\n",
      "rewards/mean:\t-6.437500000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-8.402069092\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t12.216604233\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 38 ------------------------------\n",
      "rewards/mean:\t-6.875000000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-8.582475662\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t9.522586823\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 39 ------------------------------\n",
      "rewards/mean:\t-6.156250000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-8.289455414\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t13.384156227\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 40 ------------------------------\n",
      "rewards/mean:\t-5.156250000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-7.096855164\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t12.943065643\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 41 ------------------------------\n",
      "rewards/mean:\t-5.593750000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-7.207509041\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t10.571941376\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 42 ------------------------------\n",
      "rewards/mean:\t-5.250000000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-6.963848591\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t9.827368736\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 43 ------------------------------\n",
      "rewards/mean:\t-5.312500000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-7.243252277\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t12.975872993\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 44 ------------------------------\n",
      "rewards/mean:\t-4.812500000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-6.437671661\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t10.079950333\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 45 ------------------------------\n",
      "rewards/mean:\t-4.281250000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-6.176764965\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t12.653021812\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 46 ------------------------------\n",
      "rewards/mean:\t-4.812500000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-6.391715050\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t10.911811829\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 47 ------------------------------\n",
      "rewards/mean:\t-5.031250000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-6.708403587\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t10.371419907\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 48 ------------------------------\n",
      "rewards/mean:\t-4.937500000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-6.640993118\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t11.062007904\t<---- how far we are from the original model (regularizer)\n",
      "\n",
      "------------------------------ STEP 49 ------------------------------\n",
      "rewards/mean:\t-4.687500000\t<---- average reward over this batch (higher=better, noisy)\n",
      "ppo/returns/mean:\t-6.371194839\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t12.013458252\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_steps = 50\n",
    "generation_kwargs = dict(\n",
    "    min_length=-1,\n",
    "    max_length=20,\n",
    "    do_sample=False,\n",
    "    num_beams=1,\n",
    "    temperature=1.0,\n",
    "    pad_token_id=main_tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "with tqdm(enumerate(ppo_trainer.dataloader), total=max_steps) as progressbar:\n",
    "  for epoch, batch in progressbar:\n",
    "    if epoch >= max_steps:\n",
    "        break\n",
    "\n",
    "    # Rollout stage: generate continuations from batch queries using main_model\n",
    "    response_tensors = ppo_trainer.generate(batch['input_ids'], **generation_kwargs)\n",
    "\n",
    "    # de-tokenize responses to strings (since reward model uses a different tokenizer)\n",
    "    batch[\"response\"] = [main_tokenizer.decode(response.squeeze()) for response in response_tensors]\n",
    "\n",
    "    # Evaluation stage\n",
    "    rewards = compute_reward(batch['response'])\n",
    "    \n",
    "    # Update stage\n",
    "    stats = ppo_trainer.step(batch['input_ids'], response_tensors, list(rewards.split(1)))\n",
    "    stats['rewards/mean'] = rewards.mean().item()\n",
    "\n",
    "    print(\"-\" * 30, 'STEP', epoch, '-' * 30)\n",
    "    print(f'rewards/mean:\\t{stats[\"rewards/mean\"]:.9f}\\t<---- average reward over this batch (higher=better, noisy)')\n",
    "    print(f'ppo/returns/mean:\\t{stats[\"ppo/returns/mean\"]:.9f}\\t<---- model-estimated average discounted reward')\n",
    "    print(f'objective/kl:\\t{stats[\"objective/kl\"]:.9f}\\t<---- how far we are from the original model (regularizer)')\n",
    "    print()\n",
    "\n",
    "    ppo_trainer.log_stats(stats, batch, list(rewards.split(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "9WpurLBd5NAr",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "31ad8546-3901-474e-ffda-acd042218048"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): AutoModelForCausalLMWithValueHead(\n",
       "      (pretrained_model): GPT2LMHeadModel(\n",
       "        (transformer): GPT2Model(\n",
       "          (wte): Embedding(50257, 1280)\n",
       "          (wpe): Embedding(1024, 1280)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "          (h): ModuleList(\n",
       "            (0-35): 36 x GPT2Block(\n",
       "              (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): GPT2Attention(\n",
       "                (c_attn): Linear(\n",
       "                  in_features=1280, out_features=3840, bias=True\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=32, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=32, out_features=3840, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (c_proj): Conv1D()\n",
       "                (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): GPT2MLP(\n",
       "                (c_fc): Conv1D()\n",
       "                (c_proj): Conv1D()\n",
       "                (act): NewGELUActivation()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (ln_f): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (lm_head): Linear(in_features=1280, out_features=50257, bias=False)\n",
       "      )\n",
       "      (v_head): ValueHead(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (summary): Linear(in_features=1280, out_features=1, bias=True)\n",
       "        (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#main_model.config.use_cache = True\n",
    "main_model.gradient_checkpointing_disable()\n",
    "main_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "47575484d8f041859e4f187a99382dc7",
      "e44fd70d8ba24bbaa47a192e124ebe1c",
      "5b0e09b231774db588b32bab3fd465e6",
      "2a9cf07cfccd44889a3981c33c0bdec2",
      "496aa6b7584c46fe8af1d8405b1ebaa2",
      "76de7d5250ed4d5ca7ecab8f01044dcb",
      "497e09015c4346b6bfb85d30f8f85632",
      "99292ffd235e4d39a344a2302045cf3c",
      "9abdc946ddf64545850a46a47e46fb1a",
      "94097ee8bb854c89a77c8a3971e7d1f3",
      "9670ab531e9a4d648c8f8eeea9b58155"
     ]
    },
    "id": "M_au2aV6CYqN",
    "outputId": "2955008c-64cb-4cab-b38b-3c28b0a107b4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47575484d8f041859e4f187a99382dc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rlhf_generation_lens = []\n",
    "\n",
    "for i in tqdm(range(0, len(ds_test['query']), batch_size)):\n",
    "    batch = ds_test['query'][i:i+batch_size]\n",
    "    inputs = main_tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = main_model.model.generate(\n",
    "            **inputs,\n",
    "            min_length=-1,\n",
    "            max_length=20,\n",
    "            do_sample=False,\n",
    "            num_beams=1,\n",
    "            temperature=1.0,\n",
    "            pad_token_id=main_tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    decoded_outputs = main_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    rlhf_generation_lens.extend([len(text.split()) for text in decoded_outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "XJ1pePH2Ddzx",
    "outputId": "6d736554-4f74-437c-edd9-573ffb552f8a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([non_rlhf_generation_lens, rlhf_generation_lens], bins='auto', alpha=0.7, label=['before RLHF', 'after RLHF'])\n",
    "plt.xlabel('Length of generated text')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Before and after RLHF')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6XH2oNFMD8su"
   },
   "source": [
    "As the histograms show, the distribution of generated text lengths did indeed change after rewarded training. There is no reward cheating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using RLHF we obtained a model that produces short reviews!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "4c8ff454cd947027f86954d72bf940c689a97dcc494eb53cfe4813862c6065fe"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
